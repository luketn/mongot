package com.xgen.mongot.index.lucene;

import static com.xgen.mongot.util.Check.checkState;

import com.xgen.mongot.cursor.batch.BatchCursorOptions;
import com.xgen.mongot.index.BatchProducer;
import com.xgen.mongot.index.IntermediateFacetBucket;
import com.xgen.mongot.index.query.collectors.FacetCollector;
import com.xgen.mongot.util.BsonUtils;
import com.xgen.mongot.util.Bytes;
import com.xgen.mongot.util.bson.BsonArrayBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Queue;
import org.bson.BsonArray;
import org.bson.BsonDocument;
import org.bson.BsonInt64;
import org.bson.BsonString;
import org.bson.RawBsonDocument;

/**
 * Outputs batches of intermediate meta results in response to a search against a sharded
 * collection. These intermediate results will later be merged using the plan generated by {@link
 * com.xgen.mongot.server.command.search.ShardedSearchPlanner}. It always takes in and outputs a
 * count of all document results of type "count". It also builds a {@link LuceneMetaBucketProducer}
 * for each given facet definition (only one producer for multiple string facets on the same path)
 * and outputs batches of buckets from these until they are all exhausted. This class is only used
 * in the code path of intermediateCollectorQuery().
 */
class LuceneFacetCollectorMetaBatchProducer implements BatchProducer {

  private final long totalHits;

  private final Queue<LuceneMetaBucketProducer> bucketProducers;

  private boolean countProduced;
  private boolean closed;
  private final FacetCollector facetCollector;

  LuceneFacetCollectorMetaBatchProducer(
      long totalHits,
      Queue<LuceneMetaBucketProducer> bucketProducers,
      FacetCollector facetCollector) {

    this.totalHits = totalHits;
    this.bucketProducers = bucketProducers;

    this.countProduced = false;
    this.closed = false;

    this.facetCollector = facetCollector;
  }

  public FacetCollector getFacetCollector() {
    return this.facetCollector;
  }

  @Override
  public void execute(Bytes sizeLimit, BatchCursorOptions queryCursorOptions) throws IOException {}

  @Override
  public BsonArray getNextBatch(Bytes resultsSizeLimit) throws IOException {
    checkState(!this.closed, "Cannot call getNextBatch() on closed BatchProducer.");
    BsonArrayBuilder resultsBuilder = BsonArrayBuilder.withLimit(resultsSizeLimit);

    if (!this.countProduced) {
      if (!resultsBuilder.append(getCountRawBsonDoc(this.totalHits))) {
        throw new IllegalStateException("Could not fit count bucket in meta buckets batch.");
      }
      this.countProduced = true;
    }

    while (!this.bucketProducers.isEmpty()) {
      LuceneMetaBucketProducer currProducer = this.bucketProducers.peek();
      while (!currProducer.isExhausted()) {
        if (resultsBuilder.append(currProducer.peek().toRawBson())) {
          currProducer.acceptAndAdvance();
        } else {
          return resultsBuilder.build();
        }
      }
      this.bucketProducers.remove();
    }

    return resultsBuilder.build();
  }

  /**
   * Drains all facet bucket results, but not the count. The isExhausted() will become true after
   * calling this method.
   */
  public List<IntermediateFacetBucket> getAllBucketResults() throws IOException {
    List<IntermediateFacetBucket> ret = new ArrayList<>();
    while (!this.bucketProducers.isEmpty()) {
      LuceneMetaBucketProducer currProducer = this.bucketProducers.peek();
      while (!currProducer.isExhausted()) {
        ret.add(currProducer.peek());
        currProducer.acceptAndAdvance();
      }
      this.bucketProducers.remove();
    }
    return ret;
  }

  public long getTotalHits() {
    return this.totalHits;
  }

  @Override
  public boolean isExhausted() {
    return this.bucketProducers.isEmpty();
  }

  @Override
  public void close() {
    this.closed = true;
    this.bucketProducers.clear();
  }

  public static RawBsonDocument getCountRawBsonDoc(long totalHits) {
    // Should we create a DocumentEncodable class for this document?
    return BsonUtils.documentToRaw(
        new BsonDocument("type", new BsonString("count"))
            .append("count", new BsonInt64(totalHits)));
  }
}
